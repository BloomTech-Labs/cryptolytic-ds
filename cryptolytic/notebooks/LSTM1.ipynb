{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "ERROR:root:File `'cryptolytic/notebooks/init.ipynb.py'` not found.\nc:\\Users\\kyleh\\Desktop\nC:\\Users\\kyleh\\Anaconda3\\lib\\site-packages\\dotenv\\main.py:52: UserWarning:\n\nFile doesn't exist \n\n"
    },
    {
     "data": {
      "text/html": "\n<style>\n.MathJax {\n    font-size: 2rem;\n}\n</style>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../..\n",
    "%run cryptolytic/notebooks/init.ipynb\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cryptolytic.util as util\n",
    "import cryptolytic.start as start\n",
    "import cryptolytic.viz.plot as plot\n",
    "import cryptolytic.data.sql as sql\n",
    "import cryptolytic.data as d\n",
    "from cryptolytic.util import *\n",
    "import cryptolytic.data.historical as h\n",
    "import cryptolytic.model as m\n",
    "import cryptolytic.model.lstm_framework as lstm\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "from IPython.core.display import HTML\n",
    "from pandas.plotting import register_matplotlib_converters # to stop a warning message\n",
    "\n",
    "\n",
    "ohclv = ['open', 'high', 'close', 'low', 'volume']\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = 20,7\n",
    "start.init()\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# Make math readable\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax {\n",
    "    font-size: 2rem;\n",
    "}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "### LSTM with Memory Between Batches\n",
    "\n",
    ">We can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer “stateful”. This means that it can build state over the entire training sequence and even maintain that state if needed to make predictions.\n",
    "\n",
    ">It requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that we must create our own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n",
    "\n",
    "```{python}\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "```\n",
    "\n",
    ">Finally, when the LSTM layer is constructed, the stateful parameter must be set True and instead of specifying the input dimensions, we must hard code the number of samples in a batch, number of time steps in a sample and number of features in a time step by setting the batch_input_shape parameter. For example:\n",
    "```\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, time_steps, features), stateful=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'yeojohnson' from 'scipy.stats' (C:\\Users\\kyleh\\Anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-797a016f96c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myeojohnson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mthing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'yeojohnson' from 'scipy.stats' (C:\\Users\\kyleh\\Anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "def thing(arg, axis=0):\n",
    "    x = np.sign(arg) * np.log(np.abs(arg) + 1)\n",
    "    mu = np.nanmean(x, axis=axis)\n",
    "    std = np.nanstd(x, axis=axis)\n",
    "    return x, mu, std\n",
    "\n",
    "# Version 2 \n",
    "def normalize(A):\n",
    "    if isinstance(A, pd.DataFrame) or isinstance(A, pd.Series):\n",
    "        A = A.values\n",
    "    if np.ndim(A)==1:\n",
    "        A = np.expand_dims(A, axis=1)\n",
    "    A = A.copy()\n",
    "    x, mu, std = thing(A, axis=0)\n",
    "    for i in range(A.shape[1]):\n",
    "        A[:, i] = (x[:, i] - mu[i]) / std[i]\n",
    "    return A\n",
    "   \n",
    "def denormalize(values, df, col=None):\n",
    "    values = values.copy()\n",
    "    \n",
    "    def eq(x, mu, std):\n",
    "        return np.exp((x * std) + mu) - 1\n",
    "    \n",
    "    if np.ndim(values) == 1 and col is not None:\n",
    "        x, mu, std = thing(df[col])\n",
    "        return eq(values, mu, std)\n",
    "    else:\n",
    "        for i in range(values.shape[1]): \n",
    "            x, mu, std = thing(df.iloc[:, i])\n",
    "            if isinstance(values, pd.DataFrame): \n",
    "                values.iloc[:, i] = eq(values.iloc[:, i], mu, std)\n",
    "            else:\n",
    "                values[:, i] = eq(values[:, i], mu, std) \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "history_size = 400\n",
    "input_len = 16000\n",
    "lahead = 12*3\n",
    "step = 2\n",
    "period = 300\n",
    "to_drop = lahead - 1\n",
    "input_len = input_len + to_drop\n",
    "batch_size = 200\n",
    "\n",
    "df_orig = None\n",
    "df_orig = d.get_df ({'start':'06-01-2019', 'period':period, 'trading_pair':'btc_usd', 'exchange_id':'bitfinex'},\n",
    "               n=input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta\n",
    "df = df_orig\n",
    "df = df.sort_index()\n",
    "df = df._get_numeric_data().drop([\"period\"], axis=1, errors='ignore')\n",
    "johnson = df[['volume', 'high_m_low', 'arb_signal']].apply(lambda x: yeojohnson(np.float64(x))[0]).rename(lambda x: x+'_johnson', axis=1)\n",
    "df = df.filter(regex=\"(?!timestamp_.*)\", axis=1) # filter out timestapm_ metrics\n",
    "df = ta.add_all_ta_features(df, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\").dropna(axis=1)\n",
    "df_diff = (df - df.shift(1, fill_value=0)).rename(lambda x: x+'_diff', axis=1)\n",
    "df = pd.concat([df, johnson, df_diff], axis=1)\n",
    "dataset = normalize(df.values)\n",
    "target = df.columns.get_loc('close') \n",
    "y = dataset[:, target]\n",
    "history = {'loss' : [], 'val_loss' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed(df, target, batch_size, history_size, step, lahead=1, ratio=0.8):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    x = dataset\n",
    "    y = dataset[:, target]\n",
    "\n",
    "    start = history_size # 1000\n",
    "    end = df.shape[0] - lahead # 4990\n",
    "    # 4990 - 1000 = 3990\n",
    "    for i in range(start, end):\n",
    "        # grab rows from i, to i+history_size\n",
    "        indices = range(i-history_size, i, step)\n",
    "        xs.append(x[indices])\n",
    "        ys.append(y[i:i+lahead])\n",
    "        \n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    nrows = xs.shape[0]\n",
    "    train_size = int(nrows * ratio)\n",
    "    # make sure the sizes are multiples of the batch size (needed for stateful lstm)\n",
    "    train_size -= train_size % batch_size\n",
    "    val_size = nrows - train_size\n",
    "    val_size -= val_size  % batch_size\n",
    "    total_size = train_size + val_size\n",
    "    xs = xs[:total_size]\n",
    "    ys = ys[:total_size]\n",
    "    \n",
    "    return xs[:train_size], ys[:train_size], xs[train_size:], ys[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = windowed(dataset, target, batch_size, history_size, step, lahead)\n",
    "mapl(lambda x: x.shape, [x_train, y_train, x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "print ('\\n Number of candles to predict : {}'.format(y_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# BUFFER_SIZE = 10_000\n",
    "# BATCH_SIZE = 250\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "# \n",
    "# val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "# val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import dtw\n",
    "\n",
    "# x = df['close']\n",
    "# y = df['close'].shift(1)\n",
    "# manhattan_distance = lambda x, y: np.abs(x - y)\n",
    "\n",
    "# d, cost_matrix, acc_cost_matrix, path = dtw(x, y, dist=manhattan_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For areas lacking data, can use masking \n",
    "\n",
    "inputs[:, 3, :] = 0. # for areas without a timestep, set to the mask value\n",
    "inputs[:, 5, :] = 0.\n",
    "model.add(tf.keras.layers.Masking(mask_value=0.,\n",
    "                                  input_shape=(timesteps, features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.constraints as constraints\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Activation, Add, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform, glorot_uniform\n",
    "\n",
    "\n",
    "def conv_block(X, filters, f, stage,block, strides=2):\n",
    "    \"\"\"\n",
    "    X -- input tensor\n",
    "    f -- interger specify shape of conv window\n",
    "    stage - integer, used to named layers, depending on position in network.\n",
    "    stride -- stride\n",
    "    \"\"\"\n",
    "    conv_base_name = 'conv' + str(stage) + block\n",
    "    bn_base_name = 'bn' + str(stage) + block\n",
    "    \n",
    "    f1, f2, f3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    X = Conv1D(f1, kernel_size=1, strides=strides, name=conv_base_name + '_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f2, kernel_size=f, strides=1, padding='same', name=conv_base_name + '_2', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f3, kernel_size=1, strides=1, padding='valid', name=conv_base_name + '_3', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # skip connection\n",
    "    X_shortcut = Conv1D(f3, kernel_size=1, strides=strides, padding='valid', name=conv_base_name + '_4', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    \n",
    "    assert tf.keras.backend.int_shape(X) == tf.keras.backend.int_shape(X_shortcut)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('tanh')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def identity_block(X, f, filters, stage, block, strides=2):\n",
    "    \"\"\"\n",
    "    X -- input tensor\n",
    "    f -- interger specify shape of conv window\n",
    "    stage - integer, used to named layers, depending on position in network.\n",
    "    stride -- stride\n",
    "    \"\"\"\n",
    "    conv_base_name = 'conv' + str(stage) + block\n",
    "    bn_base_name = 'bn' + str(stage) + block\n",
    "    \n",
    "    f1, f2, f3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    X = Conv1D(f1, kernel_size=1, strides=strides, name=conv_base_name + '_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f2, kernel_size=f, strides=1, padding='same', name=conv_base_name + '_2', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f3, kernel_size=1, strides=strides, padding='valid', name=conv_base_name + '_3', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # skip connection, without also convolving x_shortcut\n",
    "    assert tf.keras.backend.int_shape(X) == tf.keras.backend.int_shape(X_shortcut)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('tanh')(X)\n",
    "    return X\n",
    "\n",
    "#class attention_layer(layers.Layer):\n",
    "#    #print([var.name for var in attn_layer.trainable_variables])\n",
    "#    def __init__(self, attention_size):\n",
    "#        super(attention_layer, self).__init__()\n",
    "#        self.attention_size = attention_size\n",
    "#        self.b_omega = tf.Variable(tf.random.normal([attention_size], stddev=0.1))\n",
    "#        self.u_omega = tf.Variable(tf.random.normal([attention_size], stddev=0.1))\n",
    "#        \n",
    "#    def build(self, input_shape):\n",
    "#        self.w_omega = tf.Variable(tf.random.normal([input_shape[-1], self.attention_size], stddev=0.1))\n",
    "#        \n",
    "#    def call(self, _in):\n",
    "#        v = tf.tanh(tf.tensordot(_in, self.w_omega, axes=1) + self.b_omega)\n",
    "#        vu = tf.tensordot(v, self.u_omega, axes=1, name='vu') # B,T shape\n",
    "#        alphas = tf.nn.softmax(vu, name='alphas')\n",
    "#        \n",
    "#        output = tf.reduce_sum(_in * tf.expand_dims(alphas, -1), 1)\n",
    "#        return output\n",
    "\n",
    "class Tuner(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Tuner, self).__init__()\n",
    "        #self.classifier = Dense()\n",
    "            \n",
    "    def build(self):\n",
    "            pass\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        \n",
    "        pass\n",
    "    pass\n",
    "\n",
    "    \n",
    "def create_model(x_train, params):\n",
    "    attention_size = 5\n",
    "    input_shape = x_train.shape[-2:]\n",
    "    X_input = Input(input_shape, batch_size=batch_size)\n",
    "    X = X_input\n",
    "    X = layers.ZeroPadding1D(padding=2)(X)\n",
    "#    X = layers.TimeDistributed(Dense(input_shape[-1], kernel_constraint=constraints.max_norm(1.0), activation='tanh'))(X)\n",
    "    X = Conv1D(filters=params.filters1, kernel_size=6, strides=1, name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = layers.GaussianNoise(params.noise1)(X)\n",
    "    #X = layers.AveragePooling1D(2, strides=1)(X)\n",
    "    X = conv_block(X, f=3, filters=params.filtershape1, stage=2, block='a', strides=2)\n",
    "    X = layers.GaussianNoise(params.noise1)(X)\n",
    "    X = identity_block(X, f=3, filters=params.filtershape1, stage=2, block='b', strides=1)\n",
    "    X = layers.GaussianNoise(params.noise1)(X)\n",
    "    X = identity_block(X, f=3, filters=params.filtershape1, stage=2, block='c', strides=1)\n",
    "    \n",
    "    print(X.get_shape())\n",
    "    print(X_input.get_shape())\n",
    "    #X = layers.concatenate([X, X_input])\n",
    "    \n",
    "   # X = layers.AveragePooling1D(2, name=\"avg_pool\")(X)\n",
    "    \n",
    "#    rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]\n",
    "#    stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)\n",
    "#    lstm_layer = tf.keras.layers.RNN(stacked_lstm)\n",
    "#    X = lstm_layer(X)\n",
    " \n",
    "    X = layers.Flatten()(X)\n",
    "    X = Dense(256)(X)\n",
    "    X = Dense(lahead, kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='model1')\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptolytic.util import *\n",
    "        \n",
    "    \n",
    "def fit_model(model, inputX, inputy):\n",
    "    epochs = 5\n",
    "    # batch size higher than 1 c  epochs = 10\n",
    "    for i in range(epochs):\n",
    "        model.fit(inputX,\n",
    "                  inputy, \n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  use_multiprocessing=True,\n",
    "                  shuffle=False,\n",
    "                  workers=4,\n",
    "                  validation_data = (x_val, y_val))\n",
    "        history['loss'].append(model.history.history['loss'])\n",
    "        history['val_loss'].append(model.history.history['val_loss'])\n",
    "#        model.reset_states()\n",
    "    #pred = transformer.denormalize(model.predict(x_val)[:, 0], df, 'close')\n",
    "    #pred_history.append(pred)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def save_model(model, folder, params=None):\n",
    "    path = f'models/{folder}'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    filename = os.path.join(path, 'model_' + str(np.random.rand()) + '.h5')\n",
    "    if os.path.exists(filename):\n",
    "        return save_model(model)\n",
    "    if params is not None:\n",
    "        param_path = os.path.join(path, 'model_params.csv')\n",
    "        pd.DataFrame(params).to_csv(param_path)\n",
    "        \n",
    "    model.save(filename)\n",
    "    print('Saved model')\n",
    "    \n",
    "def load_all_models(folder):\n",
    "    models = []\n",
    "    params = []\n",
    "    path = f'models/{folder}'\n",
    "    for m in os.listdir(path):\n",
    "        if m.endswith('.csv'):\n",
    "            params.append(pd.read_csv(m))\n",
    "        if not m.endswith('.h5'):\n",
    "            continue\n",
    "        m = tf.keras.models.load_model(m)\n",
    "        models.append(m)\n",
    "        print('Loaded %s' % path)\n",
    "    return models, params\n",
    "\n",
    "def stacked_dataset(models, inputX):\n",
    "    stackX = None\n",
    "    for model in models:\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        #stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilties]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "def fit_stacked_model(models, inputX, inputy):\n",
    "    stackedX = stacked_dataset(models, inputX)\n",
    "    # fit model\n",
    "    model = create_model(inputX)\n",
    "    fit_model(model, inputX, inputy)\n",
    "    return model\n",
    "\n",
    "def stacked_prediction(models, model, inputX):\n",
    "    stackedX = stacked_dataset(models, inputX)\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat\n",
    "\n",
    "def endingly():\n",
    "    models, params = load_all_models('example')\n",
    "    model = fit_stacked_model(models, x_train, y_train)\n",
    "    preds = stacked_prediction(models, model, x_train)\n",
    "    evaluate_models(models)\n",
    "    return preds\n",
    "\n",
    "def hyperparameter(inputX, inputy):\n",
    "    filtershape1 = 32 + 16 * np.random.randint(0, 6)\n",
    "    filtershape2 = 32 + 16 * np.random.randint(0, 6)\n",
    "    \n",
    "    params = adict(\n",
    "        filters1 = 32 + 16 * np.random.randint(0, 5),\n",
    "        noise1 = np.random.uniform(high=0.01),\n",
    "        filtershape1 = [filtershape1, filtershape1, filtershape1*2],\n",
    "        filtershape2 = [filtershape2, filtershape2, filtershape2*2]\n",
    "    )\n",
    "    print(params)\n",
    "    model = create_model(inputX, params)\n",
    "    fit_model(model, inputX, inputy)\n",
    "    save_model(model, 'filter', params=params)\n",
    "    return model\n",
    "                 \n",
    "def run_tuning():\n",
    "    models = []\n",
    "    for i in range(50):\n",
    "        models.append(hyperparameter(x_train, y_train))\n",
    "    return models\n",
    "\n",
    "   \n",
    "def evaluate_models(models):\n",
    "    for m in models:\n",
    "        _, acc = model.evaluate(x_val, y_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'filters1': 32, 'noise1': 0.007044669933974564, 'filtershape1': [48, 48, 96], 'filtershape2': [64, 64, 128]}\n",
    "{'filters1': 96, 'noise1': 0.0067625880989959294, 'filtershape1': [48, 48, 96], 'filtershape2': [32, 32, 64]}\n",
    "{'filters1': 64, 'noise1': 0.002182333479015578, 'filtershape1': [32, 32, 64], 'filtershape2': [48, 48, 96]}\n",
    "{'filters1': 48, 'noise1': 0.004816923153178217, 'filtershape1': [48, 48, 96], 'filtershape2': [32, 32, 64]}\n",
    "{'filters1': 32, 'noise1': 0.0004049353180822357, 'filtershape1': [64, 64, 128], 'filtershape2': [32, 32, 64]}\n",
    "{'filters1': 64, 'noise1': 0.0015181074459694344, 'filtershape1': [48, 48, 96], 'filtershape2': [64, 64, 128]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.mean([32, 96, 64, 48, 32, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = run_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "endingly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(x_train)\n",
    "history = {'loss':[],'val_loss':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils.vis_utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "    \n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "        label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "            label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_train_history(loss, val_loss, title):\n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_train_history(history['loss'], history['val_loss'], 'Multi Step Training and validation loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = denormalize(model.predict(x_train)[:, 0], df, 'close')\n",
    "#preds = denormalize(df, transformer)\n",
    "#preds = denormalize(model.predict(x_train)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,9\n",
    "preds = model.predict(x_train)[:, 0]\n",
    "n = len(preds)\n",
    "h = history_size\n",
    "hn = n+history_size\n",
    "yo= len(x_val)+history_size\n",
    "z = yo\n",
    "modman = len(df)% batch_size\n",
    "w = z + len(df)%modman\n",
    "\n",
    "# plt.plot(range(n), d.denoise(df['close'][h:hn], 400), label='actual', color='green')\n",
    "# plt.plot(range(n), d.denoise(preds[:n], 5), label='predicted');\n",
    "# plt.plot(range(n), df.close_mean.values[h:hn], label='Mean')\n",
    "\n",
    "plt.plot(y_train[:, 0], label='Train')\n",
    "plt.plot(d.denoise(preds, 5), label='Preds')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = denormalize(model.predict(x_val)[:, 0], df, 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = denormalize(y_val[:, 0], df, 'close')\n",
    "#preds = model.predict(x_val)[:, 0]\n",
    "val_preds = model.predict(x_val)[:, 0]\n",
    "#y_actual = y_val[:, 0]\n",
    "val_n = len(val_preds)\n",
    "hn = n+history_size\n",
    "plt.plot(range(val_n), d.denoise(y_actual, 5), label='actual')\n",
    "#plt.plot(range(val_n), df['close'].iloc[-val_n:], label='actual')\n",
    "plt.plot(range(val_n), d.denoise(val_preds, 5), label='predicted');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(val_n), d.denoise(val_preds[1], 5), label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = denormalize(model.predict(x_val)[:, 0], df, 'close')\n",
    "y_actual = denormalize(y_val[:, 0], df, 'close')\n",
    "plt.plot(yo, label='predicted')\n",
    "plt.plot(y_actual, label='actual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}