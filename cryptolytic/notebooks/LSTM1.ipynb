{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "c:\\Users\\kyleh\\Desktop\\Repos\\cryptolytic-ds\nUsing matplotlib backend: Qt5Agg\nPopulating the interactive namespace from numpy and matplotlib\n"
    },
    {
     "data": {
      "text/html": "\n<style>\n.MathJax {\n    font-size: 2rem;\n}\n</style>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../..\n",
    "%run cryptolytic/notebooks/init.ipynb\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import cryptolytic.util as util\n",
    "import cryptolytic.start as start\n",
    "import cryptolytic.viz.plot as plot\n",
    "import cryptolytic.data.sql as sql\n",
    "import cryptolytic.data as d\n",
    "from cryptolytic.util import *\n",
    "import cryptolytic.data.historical as h\n",
    "import cryptolytic.model as m\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "from IPython.core.display import HTML\n",
    "from pandas.plotting import register_matplotlib_converters # to stop a warning message\n",
    "\n",
    "\n",
    "ohclv = ['open', 'high', 'close', 'low', 'volume']\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = 20,7\n",
    "start.init()\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# Make math readable\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax {\n",
    "    font-size: 2rem;\n",
    "}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "### LSTM with Memory Between Batches\n",
    "\n",
    ">We can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer “stateful”. This means that it can build state over the entire training sequence and even maintain that state if needed to make predictions.\n",
    "\n",
    ">It requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that we must create our own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n",
    "\n",
    "```{python}\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "```\n",
    "\n",
    ">Finally, when the LSTM layer is constructed, the stateful parameter must be set True and instead of specifying the input dimensions, we must hard code the number of samples in a batch, number of time steps in a sample and number of features in a time step by setting the batch_input_shape parameter. For example:\n",
    "```\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, time_steps, features), stateful=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "c:\\Users\\kyleh\\Desktop\\Repos\\cryptolytic-ds\\cryptolytic\\data\\__init__.py:54: FutureWarning:\n\nhow in .resample() is deprecated\nthe new syntax is .resample(...)..apply(<func>)\n\n"
    }
   ],
   "source": [
    "# TRAIN_SPLIT = 5000\n",
    "history_size = 1000\n",
    "input_len = 8500\n",
    "train_size = int(5000*0.8)\n",
    "lahead = 10\n",
    "step = 2\n",
    "rolling_size = 6\n",
    "period = 300\n",
    "batch_size = 100\n",
    "to_drop = max(rolling_size - 1, lahead - 1)\n",
    "input_len = input_len + to_drop\n",
    "df = d.get_df ({'start':'06-01-2019', 'period':period, 'trading_pair':'eth_btc', 'exchange_id':'bitfinex'},\n",
    "               n=input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO upgrade tensorflow to 2\n",
    "# import tensorflow_transform as tft\n",
    "# tft.pca(subdf.iloc[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>api</th>\n      <th>exchange</th>\n      <th>trading_pair</th>\n      <th>timestamp</th>\n      <th>period</th>\n      <th>open</th>\n      <th>close</th>\n      <th>high</th>\n      <th>low</th>\n      <th>volume</th>\n      <th>...</th>\n      <th>diff_std</th>\n      <th>arb_signal_std</th>\n      <th>close_skew</th>\n      <th>volume_skew</th>\n      <th>diff_skew</th>\n      <th>arb_signal_skew</th>\n      <th>close_kurt</th>\n      <th>volume_kurt</th>\n      <th>diff_kurt</th>\n      <th>arb_signal_kurt</th>\n    </tr>\n    <tr>\n      <th>datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-01-06 08:00:00</th>\n      <td>bitfinex</td>\n      <td>bitfinex</td>\n      <td>eth_btc</td>\n      <td>1546761600</td>\n      <td>300.0</td>\n      <td>0.039770</td>\n      <td>0.039770</td>\n      <td>0.039770</td>\n      <td>0.039770</td>\n      <td>53.916301</td>\n      <td>...</td>\n      <td>0.000037</td>\n      <td>0.033986</td>\n      <td>-0.568393</td>\n      <td>1.215298</td>\n      <td>0.485575</td>\n      <td>0.325941</td>\n      <td>-1.685222</td>\n      <td>0.418789</td>\n      <td>-0.756465</td>\n      <td>-0.86204</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 08:05:00</th>\n      <td>bitfinex</td>\n      <td>bitfinex</td>\n      <td>eth_btc</td>\n      <td>1546761900</td>\n      <td>300.0</td>\n      <td>0.039737</td>\n      <td>0.039735</td>\n      <td>0.039748</td>\n      <td>0.039735</td>\n      <td>4.151015</td>\n      <td>...</td>\n      <td>0.000037</td>\n      <td>0.033986</td>\n      <td>-0.568393</td>\n      <td>1.215298</td>\n      <td>0.485575</td>\n      <td>0.325941</td>\n      <td>-1.685222</td>\n      <td>0.418789</td>\n      <td>-0.756465</td>\n      <td>-0.86204</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 08:10:00</th>\n      <td>bitfinex</td>\n      <td>bitfinex</td>\n      <td>eth_btc</td>\n      <td>1546762200</td>\n      <td>300.0</td>\n      <td>0.039735</td>\n      <td>0.039760</td>\n      <td>0.039760</td>\n      <td>0.039735</td>\n      <td>85.063575</td>\n      <td>...</td>\n      <td>0.000037</td>\n      <td>0.033986</td>\n      <td>-0.568393</td>\n      <td>1.215298</td>\n      <td>0.485575</td>\n      <td>0.325941</td>\n      <td>-1.685222</td>\n      <td>0.418789</td>\n      <td>-0.756465</td>\n      <td>-0.86204</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 08:15:00</th>\n      <td>bitfinex</td>\n      <td>bitfinex</td>\n      <td>eth_btc</td>\n      <td>1546762500</td>\n      <td>300.0</td>\n      <td>0.039762</td>\n      <td>0.039690</td>\n      <td>0.039772</td>\n      <td>0.039673</td>\n      <td>413.201494</td>\n      <td>...</td>\n      <td>0.000037</td>\n      <td>0.033986</td>\n      <td>-0.568393</td>\n      <td>1.215298</td>\n      <td>0.485575</td>\n      <td>0.325941</td>\n      <td>-1.685222</td>\n      <td>0.418789</td>\n      <td>-0.756465</td>\n      <td>-0.86204</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 08:20:00</th>\n      <td>bitfinex</td>\n      <td>bitfinex</td>\n      <td>eth_btc</td>\n      <td>1546762800</td>\n      <td>300.0</td>\n      <td>0.039691</td>\n      <td>0.039755</td>\n      <td>0.039755</td>\n      <td>0.039691</td>\n      <td>78.903445</td>\n      <td>...</td>\n      <td>0.000037</td>\n      <td>0.033986</td>\n      <td>-0.568393</td>\n      <td>1.215298</td>\n      <td>0.485575</td>\n      <td>0.325941</td>\n      <td>-1.685222</td>\n      <td>0.418789</td>\n      <td>-0.756465</td>\n      <td>-0.86204</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>",
      "text/plain": "                          api  exchange trading_pair   timestamp  period  \\\ndatetime                                                                   \n2019-01-06 08:00:00  bitfinex  bitfinex      eth_btc  1546761600   300.0   \n2019-01-06 08:05:00  bitfinex  bitfinex      eth_btc  1546761900   300.0   \n2019-01-06 08:10:00  bitfinex  bitfinex      eth_btc  1546762200   300.0   \n2019-01-06 08:15:00  bitfinex  bitfinex      eth_btc  1546762500   300.0   \n2019-01-06 08:20:00  bitfinex  bitfinex      eth_btc  1546762800   300.0   \n\n                         open     close      high       low      volume  ...  \\\ndatetime                                                                 ...   \n2019-01-06 08:00:00  0.039770  0.039770  0.039770  0.039770   53.916301  ...   \n2019-01-06 08:05:00  0.039737  0.039735  0.039748  0.039735    4.151015  ...   \n2019-01-06 08:10:00  0.039735  0.039760  0.039760  0.039735   85.063575  ...   \n2019-01-06 08:15:00  0.039762  0.039690  0.039772  0.039673  413.201494  ...   \n2019-01-06 08:20:00  0.039691  0.039755  0.039755  0.039691   78.903445  ...   \n\n                     diff_std arb_signal_std  close_skew  volume_skew  \\\ndatetime                                                                \n2019-01-06 08:00:00  0.000037       0.033986   -0.568393     1.215298   \n2019-01-06 08:05:00  0.000037       0.033986   -0.568393     1.215298   \n2019-01-06 08:10:00  0.000037       0.033986   -0.568393     1.215298   \n2019-01-06 08:15:00  0.000037       0.033986   -0.568393     1.215298   \n2019-01-06 08:20:00  0.000037       0.033986   -0.568393     1.215298   \n\n                     diff_skew  arb_signal_skew  close_kurt  volume_kurt  \\\ndatetime                                                                   \n2019-01-06 08:00:00   0.485575         0.325941   -1.685222     0.418789   \n2019-01-06 08:05:00   0.485575         0.325941   -1.685222     0.418789   \n2019-01-06 08:10:00   0.485575         0.325941   -1.685222     0.418789   \n2019-01-06 08:15:00   0.485575         0.325941   -1.685222     0.418789   \n2019-01-06 08:20:00   0.485575         0.325941   -1.685222     0.418789   \n\n                     diff_kurt  arb_signal_kurt  \ndatetime                                         \n2019-01-06 08:00:00  -0.756465         -0.86204  \n2019-01-06 08:05:00  -0.756465         -0.86204  \n2019-01-06 08:10:00  -0.756465         -0.86204  \n2019-01-06 08:15:00  -0.756465         -0.86204  \n2019-01-06 08:20:00  -0.756465         -0.86204  \n\n[5 rows x 30 columns]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = df[['close', 'volume', 'diff', 'arb_signal']]\n",
    "a_df = c.rolling(6).mean().bfill().rename(columns=lambda x: x+'_mean')\n",
    "b_df = c.rolling(6).std().bfill().rename(columns=lambda x: x+'_std')\n",
    "c_df = c.rolling(6).skew().bfill().rename(columns=lambda x: x+'_skew')\n",
    "d_df = c.rolling(6).kurt().bfill().rename(columns=lambda x: x+'_kurt')\n",
    "df = pd.concat([df, a_df, b_df, c_df, d_df], axis=1).dropna(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>close</th>\n      <th>volume</th>\n      <th>diff</th>\n      <th>avg</th>\n      <th>arb_diff</th>\n      <th>arb_signal</th>\n      <th>close_mean</th>\n      <th>volume_mean</th>\n      <th>diff_mean</th>\n      <th>arb_signal_mean</th>\n      <th>...</th>\n      <th>diff_std</th>\n      <th>arb_signal_std</th>\n      <th>close_skew</th>\n      <th>volume_skew</th>\n      <th>diff_skew</th>\n      <th>arb_signal_skew</th>\n      <th>close_kurt</th>\n      <th>volume_kurt</th>\n      <th>diff_kurt</th>\n      <th>arb_signal_kurt</th>\n    </tr>\n    <tr>\n      <th>datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-01-06 15:55:00</th>\n      <td>0.039692</td>\n      <td>30.240127</td>\n      <td>0.000030</td>\n      <td>0.03966666500000000000</td>\n      <td>0.000025</td>\n      <td>0.063870</td>\n      <td>0.039715</td>\n      <td>42.870302</td>\n      <td>0.000061</td>\n      <td>0.005397</td>\n      <td>...</td>\n      <td>0.000060</td>\n      <td>0.079118</td>\n      <td>-1.520739</td>\n      <td>-0.044150</td>\n      <td>0.951284</td>\n      <td>0.527292</td>\n      <td>1.804947</td>\n      <td>-2.149039</td>\n      <td>-0.681623</td>\n      <td>-0.834049</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 16:00:00</th>\n      <td>0.039632</td>\n      <td>21.617551</td>\n      <td>0.000060</td>\n      <td>0.03958875000000000000</td>\n      <td>0.000043</td>\n      <td>0.109248</td>\n      <td>0.039696</td>\n      <td>32.602967</td>\n      <td>0.000052</td>\n      <td>0.026435</td>\n      <td>...</td>\n      <td>0.000054</td>\n      <td>0.088235</td>\n      <td>-0.491736</td>\n      <td>0.540091</td>\n      <td>1.774396</td>\n      <td>-0.223149</td>\n      <td>-2.157445</td>\n      <td>-1.643676</td>\n      <td>3.683828</td>\n      <td>-2.119237</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 16:05:00</th>\n      <td>0.039567</td>\n      <td>69.620738</td>\n      <td>0.000066</td>\n      <td>0.03955875000000000000</td>\n      <td>0.000008</td>\n      <td>0.020855</td>\n      <td>0.039668</td>\n      <td>32.765557</td>\n      <td>0.000058</td>\n      <td>0.042689</td>\n      <td>...</td>\n      <td>0.000053</td>\n      <td>0.073132</td>\n      <td>-0.223048</td>\n      <td>0.558711</td>\n      <td>1.389503</td>\n      <td>-0.571255</td>\n      <td>-1.278260</td>\n      <td>-1.593250</td>\n      <td>2.698318</td>\n      <td>-0.125454</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 16:10:00</th>\n      <td>0.039610</td>\n      <td>29.638142</td>\n      <td>0.000070</td>\n      <td>0.03959525000000000000</td>\n      <td>0.000015</td>\n      <td>0.037252</td>\n      <td>0.039645</td>\n      <td>37.035044</td>\n      <td>0.000064</td>\n      <td>0.047175</td>\n      <td>...</td>\n      <td>0.000052</td>\n      <td>0.071560</td>\n      <td>0.502473</td>\n      <td>0.625635</td>\n      <td>1.019402</td>\n      <td>-0.864654</td>\n      <td>-0.219334</td>\n      <td>-1.286994</td>\n      <td>2.091711</td>\n      <td>0.824889</td>\n    </tr>\n    <tr>\n      <th>2019-01-06 16:15:00</th>\n      <td>0.039590</td>\n      <td>17.322009</td>\n      <td>0.000032</td>\n      <td>0.03959450000000000000</td>\n      <td>-0.000005</td>\n      <td>-0.011365</td>\n      <td>0.039621</td>\n      <td>29.910087</td>\n      <td>0.000043</td>\n      <td>0.024388</td>\n      <td>...</td>\n      <td>0.000027</td>\n      <td>0.062935</td>\n      <td>0.697565</td>\n      <td>1.780054</td>\n      <td>-0.693524</td>\n      <td>-0.372850</td>\n      <td>0.913374</td>\n      <td>3.656532</td>\n      <td>-0.638285</td>\n      <td>0.348180</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>",
      "text/plain": "                        close     volume      diff                     avg  \\\ndatetime                                                                     \n2019-01-06 15:55:00  0.039692  30.240127  0.000030  0.03966666500000000000   \n2019-01-06 16:00:00  0.039632  21.617551  0.000060  0.03958875000000000000   \n2019-01-06 16:05:00  0.039567  69.620738  0.000066  0.03955875000000000000   \n2019-01-06 16:10:00  0.039610  29.638142  0.000070  0.03959525000000000000   \n2019-01-06 16:15:00  0.039590  17.322009  0.000032  0.03959450000000000000   \n\n                     arb_diff  arb_signal  close_mean  volume_mean  diff_mean  \\\ndatetime                                                                        \n2019-01-06 15:55:00  0.000025    0.063870    0.039715    42.870302   0.000061   \n2019-01-06 16:00:00  0.000043    0.109248    0.039696    32.602967   0.000052   \n2019-01-06 16:05:00  0.000008    0.020855    0.039668    32.765557   0.000058   \n2019-01-06 16:10:00  0.000015    0.037252    0.039645    37.035044   0.000064   \n2019-01-06 16:15:00 -0.000005   -0.011365    0.039621    29.910087   0.000043   \n\n                     arb_signal_mean  ...  diff_std  arb_signal_std  \\\ndatetime                              ...                             \n2019-01-06 15:55:00         0.005397  ...  0.000060        0.079118   \n2019-01-06 16:00:00         0.026435  ...  0.000054        0.088235   \n2019-01-06 16:05:00         0.042689  ...  0.000053        0.073132   \n2019-01-06 16:10:00         0.047175  ...  0.000052        0.071560   \n2019-01-06 16:15:00         0.024388  ...  0.000027        0.062935   \n\n                     close_skew  volume_skew  diff_skew  arb_signal_skew  \\\ndatetime                                                                   \n2019-01-06 15:55:00   -1.520739    -0.044150   0.951284         0.527292   \n2019-01-06 16:00:00   -0.491736     0.540091   1.774396        -0.223149   \n2019-01-06 16:05:00   -0.223048     0.558711   1.389503        -0.571255   \n2019-01-06 16:10:00    0.502473     0.625635   1.019402        -0.864654   \n2019-01-06 16:15:00    0.697565     1.780054  -0.693524        -0.372850   \n\n                     close_kurt  volume_kurt  diff_kurt  arb_signal_kurt  \ndatetime                                                                  \n2019-01-06 15:55:00    1.804947    -2.149039  -0.681623        -0.834049  \n2019-01-06 16:00:00   -2.157445    -1.643676   3.683828        -2.119237  \n2019-01-06 16:05:00   -1.278260    -1.593250   2.698318        -0.125454  \n2019-01-06 16:10:00   -0.219334    -1.286994   2.091711         0.824889  \n2019-01-06 16:15:00    0.913374     3.656532  -0.638285         0.348180  \n\n[5 rows x 22 columns]"
=======
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>diff</th>\n",
       "      <th>arb_signal</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>close_mean</th>\n",
       "      <th>volume_mean</th>\n",
       "      <th>diff_mean</th>\n",
       "      <th>arb_signal_mean</th>\n",
       "      <th>timestamp_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>close_skew</th>\n",
       "      <th>volume_skew</th>\n",
       "      <th>diff_skew</th>\n",
       "      <th>arb_signal_skew</th>\n",
       "      <th>timestamp_skew</th>\n",
       "      <th>close_kurt</th>\n",
       "      <th>volume_kurt</th>\n",
       "      <th>diff_kurt</th>\n",
       "      <th>arb_signal_kurt</th>\n",
       "      <th>timestamp_kurt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-06 06:00:00</th>\n",
       "      <td>0.039829</td>\n",
       "      <td>197.424839</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.047700</td>\n",
       "      <td>1546754400</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>127.217718</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.059587</td>\n",
       "      <td>1.546755e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269907</td>\n",
       "      <td>0.780103</td>\n",
       "      <td>-0.015923</td>\n",
       "      <td>0.478313</td>\n",
       "      <td>-11031.203129</td>\n",
       "      <td>-0.417309</td>\n",
       "      <td>-0.249835</td>\n",
       "      <td>-1.893313</td>\n",
       "      <td>-0.417291</td>\n",
       "      <td>2.032476e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06 06:05:00</th>\n",
       "      <td>0.039867</td>\n",
       "      <td>320.041091</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>-0.087709</td>\n",
       "      <td>1546754700</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>127.217718</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.059587</td>\n",
       "      <td>1.546755e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269907</td>\n",
       "      <td>0.780103</td>\n",
       "      <td>-0.015923</td>\n",
       "      <td>0.478313</td>\n",
       "      <td>-11031.203129</td>\n",
       "      <td>-0.417309</td>\n",
       "      <td>-0.249835</td>\n",
       "      <td>-1.893313</td>\n",
       "      <td>-0.417291</td>\n",
       "      <td>2.032476e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06 06:10:00</th>\n",
       "      <td>0.039870</td>\n",
       "      <td>70.079043</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.090318</td>\n",
       "      <td>1546755000</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>127.217718</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.059587</td>\n",
       "      <td>1.546755e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269907</td>\n",
       "      <td>0.780103</td>\n",
       "      <td>-0.015923</td>\n",
       "      <td>0.478313</td>\n",
       "      <td>-11031.203129</td>\n",
       "      <td>-0.417309</td>\n",
       "      <td>-0.249835</td>\n",
       "      <td>-1.893313</td>\n",
       "      <td>-0.417291</td>\n",
       "      <td>2.032476e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06 06:15:00</th>\n",
       "      <td>0.039840</td>\n",
       "      <td>18.358491</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.048922</td>\n",
       "      <td>1546755300</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>127.217718</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.059587</td>\n",
       "      <td>1.546755e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269907</td>\n",
       "      <td>0.780103</td>\n",
       "      <td>-0.015923</td>\n",
       "      <td>0.478313</td>\n",
       "      <td>-11031.203129</td>\n",
       "      <td>-0.417309</td>\n",
       "      <td>-0.249835</td>\n",
       "      <td>-1.893313</td>\n",
       "      <td>-0.417291</td>\n",
       "      <td>2.032476e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-06 06:20:00</th>\n",
       "      <td>0.039804</td>\n",
       "      <td>9.974051</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.067172</td>\n",
       "      <td>1546755600</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>127.217718</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.059587</td>\n",
       "      <td>1.546755e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269907</td>\n",
       "      <td>0.780103</td>\n",
       "      <td>-0.015923</td>\n",
       "      <td>0.478313</td>\n",
       "      <td>-11031.203129</td>\n",
       "      <td>-0.417309</td>\n",
       "      <td>-0.249835</td>\n",
       "      <td>-1.893313</td>\n",
       "      <td>-0.417291</td>\n",
       "      <td>2.032476e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close      volume      diff  arb_signal   timestamp  \\\n",
       "datetime                                                                      \n",
       "2019-01-06 06:00:00  0.039829  197.424839  0.000001   -0.047700  1546754400   \n",
       "2019-01-06 06:05:00  0.039867  320.041091  0.000063   -0.087709  1546754700   \n",
       "2019-01-06 06:10:00  0.039870   70.079043  0.000001   -0.090318  1546755000   \n",
       "2019-01-06 06:15:00  0.039840   18.358491  0.000030   -0.048922  1546755300   \n",
       "2019-01-06 06:20:00  0.039804    9.974051  0.000037   -0.067172  1546755600   \n",
       "\n",
       "                     close_mean  volume_mean  diff_mean  arb_signal_mean  \\\n",
       "datetime                                                                   \n",
       "2019-01-06 06:00:00    0.039841   127.217718   0.000033        -0.059587   \n",
       "2019-01-06 06:05:00    0.039841   127.217718   0.000033        -0.059587   \n",
       "2019-01-06 06:10:00    0.039841   127.217718   0.000033        -0.059587   \n",
       "2019-01-06 06:15:00    0.039841   127.217718   0.000033        -0.059587   \n",
       "2019-01-06 06:20:00    0.039841   127.217718   0.000033        -0.059587   \n",
       "\n",
       "                     timestamp_mean  ...  close_skew  volume_skew  diff_skew  \\\n",
       "datetime                             ...                                       \n",
       "2019-01-06 06:00:00    1.546755e+09  ...   -0.269907     0.780103  -0.015923   \n",
       "2019-01-06 06:05:00    1.546755e+09  ...   -0.269907     0.780103  -0.015923   \n",
       "2019-01-06 06:10:00    1.546755e+09  ...   -0.269907     0.780103  -0.015923   \n",
       "2019-01-06 06:15:00    1.546755e+09  ...   -0.269907     0.780103  -0.015923   \n",
       "2019-01-06 06:20:00    1.546755e+09  ...   -0.269907     0.780103  -0.015923   \n",
       "\n",
       "                     arb_signal_skew  timestamp_skew  close_kurt  volume_kurt  \\\n",
       "datetime                                                                        \n",
       "2019-01-06 06:00:00         0.478313   -11031.203129   -0.417309    -0.249835   \n",
       "2019-01-06 06:05:00         0.478313   -11031.203129   -0.417309    -0.249835   \n",
       "2019-01-06 06:10:00         0.478313   -11031.203129   -0.417309    -0.249835   \n",
       "2019-01-06 06:15:00         0.478313   -11031.203129   -0.417309    -0.249835   \n",
       "2019-01-06 06:20:00         0.478313   -11031.203129   -0.417309    -0.249835   \n",
       "\n",
       "                     diff_kurt  arb_signal_kurt  timestamp_kurt  \n",
       "datetime                                                         \n",
       "2019-01-06 06:00:00  -1.893313        -0.417291    2.032476e+11  \n",
       "2019-01-06 06:05:00  -1.893313        -0.417291    2.032476e+11  \n",
       "2019-01-06 06:10:00  -1.893313        -0.417291    2.032476e+11  \n",
       "2019-01-06 06:15:00  -1.893313        -0.417291    2.032476e+11  \n",
       "2019-01-06 06:20:00  -1.893313        -0.417291    2.032476e+11  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "df_sub = df.drop(['timestamp', 'period', 'open', 'high', 'low', 'api', 'exchange', 'trading_pair'], axis=1)\n",
    "df_sub.head(100).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams['figure.figsize'] = 20,80\n",
    "# df.plot(subplots=True);"
=======
    "c = df[['close', 'volume', 'diff', 'arb_signal', 'timestamp']]\n",
    "a_df = c.rolling(rolling_size).mean().bfill().rename(columns=lambda x: x+'_mean')\n",
    "b_df = c.rolling(rolling_size).std().bfill().rename(columns=lambda x: x+'_std')\n",
    "c_df = c.rolling(rolling_size).skew().bfill().rename(columns=lambda x: x+'_skew')\n",
    "d_df = c.rolling(rolling_size).kurt().bfill().rename(columns=lambda x: x+'_kurt')\n",
    "df = pd.concat([c, a_df, b_df, c_df, d_df], axis=1).dropna(axis=1)\n",
    "# df_sub = df.drop(['timestamp', 'period', 'open', 'high', 'low', 'api', 'exchange', 'trading_pair'], axis=1)\n",
    "df.head()"
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 4,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    df = df.copy()\n",
    "    df = df._get_numeric_data()\n",
    "    # normalize each point by dividing by the inital point and subtracting 1\n",
    "    for col in df.columns: \n",
    "        df[col] = (df[col] / (df[col][0] + 1e-7)) - 1\n",
    "    return df\n",
    "    \n",
    "def denormalize(values, df, col=None):\n",
    "    values = values.copy()\n",
    "    if np.ndim(values) == 1:\n",
    "        return (1 + values) * df[col][0]\n",
    "    else:\n",
    "        for i in range(values.shape[1]):\n",
    "            if isinstance(values, pd.DataFrame): \n",
    "                values.iloc[:, i] = (1 + values.iloc[:, i]) * df.iloc[:, i][0]\n",
    "            else:\n",
    "                values[i] = (1 + values[:, i]) * df.iloc[:, i][0]\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 5,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "# dataset = normalize_df(df._get_numeric_data()).values\n",
    "dataset = normalize(df._get_numeric_data())\n",
    "target = dataset.columns.get_loc('close') \n",
    "dataset = dataset.values\n",
    "y = dataset[:, target]\n",
    "\n",
    "# x_train, y_train = multivariate_data(dataset, y, 0, TRAIN_SPLIT, past_history, lahead, STEP, single_step=False)\n",
    "# x_val, y_val = multivariate_data(dataset, y,TRAIN_SPLIT, None, past_history,lahead, STEP,single_step=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 6,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed(df, target, batch_size, history_size, step, lahead=1, ratio=0.8):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    x = dataset\n",
    "    y = dataset[:, target]\n",
    "\n",
    "    start = history_size # 1000\n",
    "    end = df.shape[0] - lahead # 4990\n",
    "    # 4990 - 1000 = 3990\n",
    "    for i in range(start, end):\n",
    "        # grab rows from i, to i+history_size\n",
    "        indices = range(i-history_size, i, step)\n",
    "        xs.append(x[indices])\n",
    "        ys.append(y[i:i+lahead])\n",
    "        \n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    nrows = xs.shape[0]\n",
    "    train_size = int(nrows * ratio)\n",
    "    # make sure the sizes are multiples of the batch size (needed for stateful lstm)\n",
    "    train_size -= train_size % batch_size\n",
    "    val_size = nrows - train_size\n",
    "    val_size -= val_size  % batch_size\n",
    "    total_size = train_size + val_size\n",
    "    xs = xs[:total_size]\n",
    "    ys = ys[:total_size]\n",
    "    \n",
    "    return xs[:train_size], ys[:train_size], xs[train_size:], ys[train_size:]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 7,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
      "text/plain": "15000"
     },
     "execution_count": 10,
=======
      "text/plain": [
       "[(5900, 500, 25), (5900, 10), (1500, 500, 25), (1500, 10)]"
      ]
     },
     "execution_count": 7,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val = windowed(dataset, target, batch_size, history_size, step, lahead)\n",
    "mapl(lambda x: x.shape, [x_train, y_train, x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 8,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< HEAD
      "text/plain": "((2280, 240, 26), (2280, 5))"
     },
     "execution_count": 11,
=======
      "text/plain": [
       "array([ 4.08857092e-05,  2.00505330e-05,  7.21719965e-06, -5.44946701e-06,\n",
       "       -1.99528456e-05, -1.89525385e-05,  1.20579045e-05, -3.61306223e-06,\n",
       "        1.88958450e-05,  1.90649688e-05])"
      ]
     },
     "execution_count": 8,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is a slight error between the normlized values and the original values\n",
    "df.iloc[1000:1000+len(y_train[0])]['close_mean'].values - denormalize(y_train[0], df, 'close_mean')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "#                      target_size, step, single_step=False):\n",
    "#    data = []\n",
    "#    labels = []\n",
    "#    \n",
    "#    start_index = start_index + history_size\n",
    "#    if end_index is None:\n",
    "#        end_index = len(dataset) - target_size\n",
    "#    \n",
    "#    for i in range(start_index, end_index):\n",
    "#        indices = range(i-history_size, i, step)\n",
    "#        data.append(dataset[indices])\n",
    "#        \n",
    "#        if single_step:\n",
    "#            labels.append(target[i+target_size])\n",
    "#        else:\n",
    "#            labels.append(target[i:i+target_size])\n",
    "#    \n",
    "#    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
<<<<<<< HEAD
     "text": "Single window of past history : (240, 26)\n\n Target temperature to predict : (5,)\n"
=======
     "text": [
      "Single window of past history : (500, 25)\n",
      "\n",
      " Target temperature to predict : (10,)\n"
     ]
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
    }
   ],
   "source": [
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "print ('\\n Target temperature to predict : {}'.format(y_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(240, 26)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single window of past history : (500, 25)\n",
      "\n",
      " Target temperature to predict : (10,)\n"
     ]
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
    }
   ],
   "source": [
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "print ('\\n Target temperature to predict : {}'.format(y_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 12,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n",
      "/home/me/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning:\n",
      "\n",
      "Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 250\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": 13,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def lstm_model(train, stateful):\n",
    "    model = tf.keras.models.Sequential()\n",
    "# use_bias is True\n",
    "    # batch size is 240 for the dataset instead of 256 for some reason\n",
    "    model.add(layers.LSTM(5, return_sequences=True, input_shape=(train.shape[-2:])))\n",
    "    model.add(layers.LSTM(5, activation='relu'))\n",
    "              \n",
    "    model.add(layers.Dense(lahead)) # global variable remove\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as models\n",
    "def create_model(df, stateful):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(batch_size=batch_size, input_shape=x_train.shape[-2:]))\n",
    "    model.add(layers.Dense(128, use_bias=False))\n",
    "    model.add(layers.Dropout(0.05)) \n",
    "    model.add(layers.LayerNormalization())\n",
    "    model.add(layers.PReLU())\n",
    " #   model.add(layers.LSTM(64, \n",
    " #                  batch_size=batch_size,\n",
    " #                  input_shape=x_train.shape[-2:],\n",
    " #                  stateful=stateful,\n",
    " #                  return_sequences=True)\n",
    " #            )\n",
    "    model.add(layers.Dropout(0.05))\n",
    "    # Time Distributed\n",
    "    # Applies a layer to every temporal slice of an input. \n",
    "    # model.add(TimeDistributed(Dense(16)))\n",
    "    model.add(layers.LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(layers.LSTM(64, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.LayerNormalization())\n",
    "    model.add(layers.Dropout(0.05))\n",
    "    model.add(layers.PReLU())\n",
    "    model.add(layers.Dense(126, activation='relu'))\n",
    "    model.add(layers.Dense(lahead))\n",
    "    #model.compile(loss='mse', optimizer='adam') \n",
    "    model.compile(loss='mae', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(df, stateful=True)\n",
    "history = {'loss':[],\n",
    "          'val_loss':[]}"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_step_history = single_step_model.fit(train_data, epochs=7,\n",
    "#                                             steps_per_epoch=38,\n",
    "#                                             shuffle=False)                                            \n",
    "##                                             validation_data=(x_val_single, y_val_single))\n",
    "\n",
    "model = lstm_model(x_train)\n",
    "\n",
    "# history = model.fit(train_data,\n",
    "#                     epochs=9, \n",
    "#                     steps_per_epoch=38,\n",
    "#                     use_multiprocessing=True,\n",
    "#                     workers=4,\n",
    "#                     validation_data=val_data,\n",
    "#                     validation_steps=5) #(x_val, y_val))"
=======
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train on 5900 samples, validate on 1500 samples\n",
      "5900/5900 [==============================] - 71s 12ms/sample - loss: 0.0873 - val_loss: 0.0117\n",
      "Epoch 1\n",
      "Train on 5900 samples, validate on 1500 samples\n",
      "5900/5900 [==============================] - 68s 12ms/sample - loss: 0.1722 - val_loss: 0.0596\n",
      "Epoch 2\n",
      "Train on 5900 samples, validate on 1500 samples\n",
      "5900/5900 [==============================] - 67s 11ms/sample - loss: 0.0726 - val_loss: 0.0163\n",
      "Epoch 3\n",
      "Train on 5900 samples, validate on 1500 samples\n",
      "5900/5900 [==============================] - 68s 12ms/sample - loss: 0.1606 - val_loss: 0.0578\n",
      "Epoch 4\n",
      "Train on 5900 samples, validate on 1500 samples\n",
      "5900/5900 [==============================] - 66s 11ms/sample - loss: 0.1228 - val_loss: 0.0248\n"
     ]
    }
   ],
   "source": [
    "def run_model():\n",
    "    epochs = 10\n",
    "    for i in range(epochs):\n",
    "        print(f'Epoch {i}')\n",
    "        # batch size higher than 1 causes to fail, not sure\n",
    "        model.fit(x_train, y_train, \n",
    "                           batch_size=batch_size,\n",
    "                           epochs=1,\n",
    "                           verbose=1,\n",
    "                           use_multiprocessing=True,\n",
    "                           workers=4,\n",
    "                           validation_data = (x_val, y_val), \n",
    "                           shuffle=False)\n",
    "        history['loss'].append(model.history.history['loss'])\n",
    "        history['val_loss'].append(model.history.history['val_loss'])\n",
    "        model.reset_states()\n",
    "        \n",
    "    return model\n",
    "        \n",
    "model = run_model()"
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 0,
=======
   "execution_count": null,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 0,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 10_000\n",
    "# BATCH_SIZE = 100\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "# \n",
    "# val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "# val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "# \n",
    "# history = model.fit(train_data,\n",
    "#                 steps_per_epoch=38,\n",
    "#                 shuffle=False,     \n",
    "#                 epochs=1,\n",
    "#                 verbose=1,\n",
    "#                 use_multiprocessing=True,\n",
    "#                 workers=4,\n",
    "#                 validation_data=val_data,\n",
    "#                 validation_steps=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "    \n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "        label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "            label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_train_history(history, 'Multi Step Training and validation loss') "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12277464928515887]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "source": [
    "history.history['loss']\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 0,
=======
   "cell_type": "markdown",
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "source": [
    "# Predictions on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = denormalize(model.predict(x_train)[:, 0], df, 'close_mean')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 0,
=======
   "execution_count": null,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,3\n",
    "yo = 1000\n",
    "plt.plot(np.arange(yo), d.denoise(df['close'].iloc[history_size:yo+history_size], 5), label='actual')\n",
    "plt.plot(range(yo), d.denoise(preds[:yo], 5), label='predicted');\n",
    "plt.plot(range(yo), df.close_mean.values[history_size:yo+history_size], label='Mean')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 0,
=======
   "execution_count": null,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('All predictions')\n",
    "# plt.plot(range(2000), d.denoise(preds[:2000], 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on validation set"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 0,
=======
   "execution_count": null,
>>>>>>> 94f15c317a76e5506c073e983d7dd6e21830006e
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = 1000\n",
    "val_actual = df['close_mean'].iloc[len(x_train):]\n",
    "val_preds = denormalize(model.predict(x_val)[:, 0], df, 'close_mean')\n",
    "plt.plot(np.arange(yo), d.denoise(val_actual, 5), label='actual')\n",
    "plt.plot(range(yo), d.denoise(preds[:yo], 5), label='predicted');\n",
    "# plt.plot(range(yo), df.close_mean.values[history_size:yo+history_size], label='Mean')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_preds = denormalize_results(model.predict(x_val))\n",
    "# # val_actual = df.close.iloc[TRAIN_SPLIT:]\n",
    "# val_actual = denormalize_results(y_val[:, 0])\n",
    "# plt.plot(np.arange(2000), d.denoise(val_actual[past_history:2000+past_history], 20), label='actual')\n",
    "# plt.plot(range(2000), d.denoise(val_preds[:, 0][:2000], 20), label='predicted');\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('kyleh': virtualenv)",
   "language": "python",
   "name": "python37364bitkylehvirtualenv5ffc9c15268948a9967a6b1571fb00cd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}