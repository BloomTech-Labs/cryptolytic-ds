{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%run cryptolytic/notebooks/init.ipynb\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import cryptolytic.util as util\n",
    "import cryptolytic.start as start\n",
    "import cryptolytic.viz.plot as plot\n",
    "import cryptolytic.data.sql as sql\n",
    "import cryptolytic.data as d\n",
    "from cryptolytic.util import *\n",
    "import cryptolytic.data.historical as h\n",
    "import cryptolytic.model as m\n",
    "import cryptolytic.model.lstm_framework as lstm\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "from IPython.core.display import HTML\n",
    "from pandas.plotting import register_matplotlib_converters # to stop a warning message\n",
    "\n",
    "\n",
    "ohclv = ['open', 'high', 'close', 'low', 'volume']\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = 20,7\n",
    "start.init()\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# Make math readable\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax {\n",
    "    font-size: 2rem;\n",
    "}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "### LSTM with Memory Between Batches\n",
    "\n",
    ">We can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer “stateful”. This means that it can build state over the entire training sequence and even maintain that state if needed to make predictions.\n",
    "\n",
    ">It requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that we must create our own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n",
    "\n",
    "```{python}\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "```\n",
    "\n",
    ">Finally, when the LSTM layer is constructed, the stateful parameter must be set True and instead of specifying the input dimensions, we must hard code the number of samples in a batch, number of time steps in a sample and number of features in a time step by setting the batch_input_shape parameter. For example:\n",
    "```\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, time_steps, features), stateful=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_SPLIT = 5000\n",
    "history_size = 1000\n",
    "input_len = 8500\n",
    "train_size = int(5000*0.8)\n",
    "lahead = 10\n",
    "step = 2\n",
    "rolling_size = 6\n",
    "period = 300\n",
    "batch_size = 100\n",
    "to_drop = max(rolling_size - 1, lahead - 1)\n",
    "input_len = input_len + to_drop\n",
    "df = d.get_df ({'start':'06-01-2019', 'period':period, 'trading_pair':'btc_usd', 'exchange_id':'bitfinex'},\n",
    "               n=input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "epsilon = 1e-9\n",
    "\n",
    "# Version 2 \n",
    "def normalize(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in df.columns: \n",
    "        # equation, (x - mu)/2sigma\n",
    "        mu = np.mean(np.log(df[col]))\n",
    "        sigma = np.sqrt(np.mean(np.log(df[col])**2) - mu**2)\n",
    "        df[col] = (np.log(df[col]) - mu) / sigma\n",
    "    return df\n",
    "   \n",
    "def denormalize(values, df, col=None):\n",
    "    values = values.copy()\n",
    "    if np.ndim(values) == 1 and col is not None:\n",
    "        mu = np.mean(np.log(df[col]))\n",
    "        sigma = np.sqrt(np.mean(np.log(df[col])**2) - mu**2)\n",
    "        return (df[col] * sigma) + mu\n",
    "    else:\n",
    "        for i in range(values.shape[1]): \n",
    "            \n",
    "            mu = np.mean(np.log(df.iloc[:, i]))\n",
    "            sigma = np.sqrt(np.mean(np.log(df.iloc[:, i])**2) - mu**2)\n",
    "            # equation, (x  * (0.5sigma - epsilon)) + mu\n",
    "            eq = lambda x, i: (x * (sigma - epsilon)) + mu\n",
    "            if isinstance(values, pd.DataFrame): \n",
    "                values.iloc[:, i] = eq(values.iloc[:, i], i)\n",
    "            else:\n",
    "                values[:, i] = eq(values[:, i], i) \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()\n",
    "normed = normalize(df._get_numeric_data())\n",
    "c = normed[['close', 'volume', 'diff', 'arb_signal', 'timestamp']]\n",
    "a_df = c.ewm(alpha=0.9).mean().bfill().rename(columns=lambda x: x+'_mean')\n",
    "b_df = c.rolling(rolling_size).std().bfill().rename(columns=lambda x: x+'_std')\n",
    "c_df = c.ewm(alpha=0.9).std().bfill().rename(columns=lambda x: x+'_ewmstd')\n",
    "d_df = c.rolling(rolling_size).skew().bfill().rename(columns=lambda x: x+'_skew')\n",
    "e_df = c.rolling(rolling_size).kurt().bfill().rename(columns=lambda x: x+'_kurt')\n",
    "f_df = c.ewm(alpha=0.9).mean().bfill().rename(columns=lambda x: x+'_ewmean') \n",
    "df = pd.concat([c, a_df, b_df, c_df, d_df, e_df, f_df], axis=1).dropna(axis=1)\n",
    "# df_sub = df.drop(['timestamp', 'period', 'open', 'high', 'low', 'api', 'exchange', 'trading_pair'], axis=1)\n",
    "df.head()\n",
    "dataset = df\n",
    "target = dataset.columns.get_loc('close') \n",
    "dataset = dataset.values\n",
    "y = dataset[:, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed(df, target, batch_size, history_size, step, lahead=1, ratio=0.8):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    x = dataset\n",
    "    y = dataset[:, target]\n",
    "\n",
    "    start = history_size # 1000\n",
    "    end = df.shape[0] - lahead # 4990\n",
    "    # 4990 - 1000 = 3990\n",
    "    for i in range(start, end):\n",
    "        # grab rows from i, to i+history_size\n",
    "        indices = range(i-history_size, i, step)\n",
    "        xs.append(x[indices])\n",
    "        ys.append(y[i:i+lahead])\n",
    "        \n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    nrows = xs.shape[0]\n",
    "    train_size = int(nrows * ratio)\n",
    "    # make sure the sizes are multiples of the batch size (needed for stateful lstm)\n",
    "    train_size -= train_size % batch_size\n",
    "    val_size = nrows - train_size\n",
    "    val_size -= val_size  % batch_size\n",
    "    total_size = train_size + val_size\n",
    "    xs = xs[:total_size]\n",
    "    ys = ys[:total_size]\n",
    "    \n",
    "    return xs[:train_size], ys[:train_size], xs[train_size:], ys[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = windowed(dataset, target, batch_size, history_size, step, lahead)\n",
    "mapl(lambda x: x.shape, [x_train, y_train, x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "print ('\\n Target temperature to predict : {}'.format(y_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 250\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def lstm_model(train, stateful):\n",
    "    model = tf.keras.models.Sequential()\n",
    "# use_bias is True\n",
    "    # batch size is 240 for the dataset instead of 256 for some reason\n",
    "    model.add(layers.LSTM(5, return_sequences=True, input_shape=(train.shape[-2:])))\n",
    "    model.add(layers.LSTM(5, activation='relu'))\n",
    "              \n",
    "    model.add(layers.Dense(lahead)) # global variable remove\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "def create_model(df, stateful):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(batch_size=batch_size, input_shape=x_train.shape[-2:]))\n",
    "    model.add(layers.LSTM(32, return_sequences=True, kernel_regularizer=ergularizers.l2(0.01))\n",
    "    model.add(layers.LSTM(32, return_sequences=True), kernel_regularizer=ergularizers.l2(0.01))\n",
    "    model.add(layers.LSTM(32, return_sequences=True), kernel_regularizer=ergularizers.l2(0.01))\n",
    "    model.add(layers.LSTM(32))\n",
    "   \n",
    "    model.add(layers.Dense(lahead))\n",
    "    #model.compile(loss='mse', optimizer='adam') \n",
    "    model.compile(loss='mae', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(df, stateful=True)\n",
    "history = {'loss':[],\n",
    "          'val_loss':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    epochs = 10\n",
    "    for i in range(epochs):\n",
    "        print(f'Epoch {i}')\n",
    "        # batch size higher than 1 causes to fail, not sure\n",
    "        model.fit(x_train, y_train, \n",
    "                           batch_size=batch_size,\n",
    "                           epochs=1,\n",
    "                           verbose=1,\n",
    "                           use_multiprocessing=True,\n",
    "                           workers=4,\n",
    "                           validation_data = (x_val, y_val), \n",
    "                           shuffle=False)\n",
    "        history['loss'].append(model.history.history['loss'])\n",
    "        history['val_loss'].append(model.history.history['val_loss'])\n",
    "        model.reset_states()\n",
    "        \n",
    "    return model\n",
    "        \n",
    "model = run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = 10_000\n",
    "# BATCH_SIZE = 100\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "# \n",
    "# val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "# val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "# \n",
    "# history = model.fit(train_data,\n",
    "#                 steps_per_epoch=38,\n",
    "#                 shuffle=False,     \n",
    "#                 epochs=1,\n",
    "#                 verbose=1,\n",
    "#                 use_multiprocessing=True,\n",
    "#                 workers=4,\n",
    "#                 validation_data=val_data,\n",
    "#                 validation_steps=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "    \n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "        label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "            label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_train_history(history, title):\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    \n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_train_history(history, 'Multi Step Training and validation loss') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = denormalize(model.predict(x_train)[:, 0], df, 'close_mean')\n",
    "preds = model.predict(x_train)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(preds)\n",
    "plt.plot(range(n), df['close'][-n-len(x_val):-len(x_val)])\n",
    "#plt.plot(range(n), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(preds)\n",
    "yo= len(x_val)+history_size\n",
    "z = yo\n",
    "modman = len(df)% batch_size\n",
    "w = z + len(df)%modman\n",
    "len(preds), len(df)-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])[1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(denormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(denormalize(y_train[:, 0], df))\n",
    "#plt.plot(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,3\n",
    "h = history_size\n",
    "hn = n+history_size\n",
    "\n",
    "plt.plot(range(n), d.denoise(df['close'][h:hn], 5), label='actual')\n",
    "plt.plot(range(n), d.denoise(preds[:n], 5), label='predicted');\n",
    "plt.plot(range(n), df.close_mean.values[h:hn], label='Mean')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('All predictions')\n",
    "# plt.plot(range(2000), d.denoise(preds[:2000], 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['close'][hn:]),val_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(x_val)[:, 0]\n",
    "val_n = len(val_preds)\n",
    "hn = n+history_size\n",
    "plt.plot(range(val_n), d.denoise(y_val[:, 0], 5), label='actual')\n",
    "plt.plot(range(val_n), d.denoise(val_preds[:n], 5), label='predicted');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_preds = denormalize_results(model.predict(x_val))\n",
    "# # val_actual = df.close.iloc[TRAIN_SPLIT:]\n",
    "# val_actual = denormalize_results(y_val[:, 0])\n",
    "# plt.plot(np.arange(2000), d.denoise(val_actual[past_history:2000+past_history], 20), label='actual')\n",
    "# plt.plot(range(2000), d.denoise(val_preds[:, 0][:2000], 20), label='predicted');\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
