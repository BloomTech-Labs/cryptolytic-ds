{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "ERROR:root:File `'cryptolytic/notebooks/init.ipynb.py'` not found.\nc:\\Users\\kyleh\\Desktop\nC:\\Users\\kyleh\\Anaconda3\\lib\\site-packages\\dotenv\\main.py:52: UserWarning:\n\nFile doesn't exist \n\n"
    },
    {
     "data": {
      "text/html": "\n<style>\n.MathJax {\n    font-size: 2rem;\n}\n</style>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../..\n",
    "%run cryptolytic/notebooks/init.ipynb\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cryptolytic.util as util\n",
    "import cryptolytic.start as start\n",
    "import cryptolytic.viz.plot as plot\n",
    "import cryptolytic.data.sql as sql\n",
    "import cryptolytic.data as d\n",
    "from cryptolytic.util import *\n",
    "import cryptolytic.data.historical as h\n",
    "import cryptolytic.model as m\n",
    "import cryptolytic.model.lstm_framework as lstm\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "from IPython.core.display import HTML\n",
    "from pandas.plotting import register_matplotlib_converters # to stop a warning message\n",
    "\n",
    "\n",
    "ohclv = ['open', 'high', 'close', 'low', 'volume']\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = 20,7\n",
    "start.init()\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "# Make math readable\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax {\n",
    "    font-size: 2rem;\n",
    "}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "### LSTM with Memory Between Batches\n",
    "\n",
    ">We can gain finer control over when the internal state of the LSTM network is cleared in Keras by making the LSTM layer “stateful”. This means that it can build state over the entire training sequence and even maintain that state if needed to make predictions.\n",
    "\n",
    ">It requires that the training data not be shuffled when fitting the network. It also requires explicit resetting of the network state after each exposure to the training data (epoch) by calls to model.reset_states(). This means that we must create our own outer loop of epochs and within each epoch call model.fit() and model.reset_states(). For example:\n",
    "\n",
    "```{python}\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "```\n",
    "\n",
    ">Finally, when the LSTM layer is constructed, the stateful parameter must be set True and instead of specifying the input dimensions, we must hard code the number of samples in a batch, number of time steps in a sample and number of features in a time step by setting the batch_input_shape parameter. For example:\n",
    "```\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, time_steps, features), stateful=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'yeojohnson' from 'scipy.stats' (C:\\Users\\kyleh\\Anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-797a016f96c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myeojohnson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mthing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'yeojohnson' from 'scipy.stats' (C:\\Users\\kyleh\\Anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "def thing(arg, axis=0):\n",
    "    x = np.sign(arg) * np.log(np.abs(arg) + 1)\n",
    "    mu = np.nanmean(x, axis=axis)\n",
    "    std = np.nanstd(x, axis=axis)\n",
    "    return x, mu, std\n",
    "\n",
    "# Version 2 \n",
    "def normalize(A):\n",
    "    if isinstance(A, pd.DataFrame) or isinstance(A, pd.Series):\n",
    "        A = A.values\n",
    "    if np.ndim(A)==1:\n",
    "        A = np.expand_dims(A, axis=1)\n",
    "    A = A.copy()\n",
    "    x, mu, std = thing(A, axis=0)\n",
    "    for i in range(A.shape[1]):\n",
    "        A[:, i] = (x[:, i] - mu[i]) / std[i]\n",
    "    return A\n",
    "   \n",
    "def denormalize(values, df, col=None):\n",
    "    values = values.copy()\n",
    "    \n",
    "    def eq(x, mu, std):\n",
    "        return np.exp((x * std) + mu) - 1\n",
    "    \n",
    "    if np.ndim(values) == 1 and col is not None:\n",
    "        x, mu, std = thing(df[col])\n",
    "        return eq(values, mu, std)\n",
    "    else:\n",
    "        for i in range(values.shape[1]): \n",
    "            x, mu, std = thing(df.iloc[:, i])\n",
    "            if isinstance(values, pd.DataFrame): \n",
    "                values.iloc[:, i] = eq(values.iloc[:, i], mu, std)\n",
    "            else:\n",
    "                values[:, i] = eq(values[:, i], mu, std) \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "history_size = 400\n",
    "input_len = 16000\n",
    "lahead = 12*3\n",
    "step = 2\n",
    "period = 300\n",
    "to_drop = lahead - 1\n",
    "input_len = input_len +\n",
    "batch_size = 200\n",
    "\n",
    "df_orig = None\n",
    "df_orig = d.get_df ({'start':'06-01-2019', 'period':period, 'trading_pair':'btc_usd', 'exchange_id':'bitfinex'},\n",
    "               n=input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta\n",
    "df = df_orig\n",
    "df = df.sort_index()\n",
    "df = df._get_numeric_data().drop([\"period\"], axis=1, errors='ignore')\n",
    "johnson = df[['volume', 'high_m_low', 'arb_signal']].apply(lambda x: yeojohnson(np.float64(x))[0]).rename(lambda x: x+'_johnson', axis=1)\n",
    "df = df.filter(regex=\"(?!timestamp_.*)\", axis=1) # filter out timestapm_ metrics\n",
    "df = ta.add_all_ta_features(df, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\").dropna(axis=1)\n",
    "df_diff = (df - df.shift(1, fill_value=0)).rename(lambda x: x+'_diff', axis=1)\n",
    "df = pd.concat([df, johnson, df_diff], axis=1)\n",
    "dataset = normalize(df.values)\n",
    "target = df.columns.get_loc('close') \n",
    "y = dataset[:, target]\n",
    "history = {'loss' : [], 'val_loss' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed(df, target, batch_size, history_size, step, lahead=1, ratio=0.8):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    x = dataset\n",
    "    y = dataset[:, target]\n",
    "\n",
    "    start = history_size # 1000\n",
    "    end = df.shape[0] - lahead # 4990\n",
    "    # 4990 - 1000 = 3990\n",
    "    for i in range(start, end):\n",
    "        # grab rows from i, to i+history_size\n",
    "        indices = range(i-history_size, i, step)\n",
    "        xs.append(x[indices])\n",
    "        ys.append(y[i:i+lahead])\n",
    "        \n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    nrows = xs.shape[0]\n",
    "    train_size = int(nrows * ratio)\n",
    "    # make sure the sizes are multiples of the batch size (needed for stateful lstm)\n",
    "    train_size -= train_size % batch_size\n",
    "    val_size = nrows - train_size\n",
    "    val_size -= val_size  % batch_size\n",
    "    total_size = train_size + val_size\n",
    "    xs = xs[:total_size]\n",
    "    ys = ys[:total_size]\n",
    "    \n",
    "    return xs[:train_size], ys[:train_size], xs[train_size:], ys[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = windowed(dataset, target, batch_size, history_size, step, lahead)\n",
    "mapl(lambda x: x.shape, [x_train, y_train, x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 0,
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2626c2253bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Single window of past history : {}'.format(x_train[0].shape))\n",
    "print ('\\n Number of candles to predict : {}'.format(y_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# BUFFER_SIZE = 10_000\n",
    "# BATCH_SIZE = 250\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "# \n",
    "# val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "# val_data = val_data.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import dtw\n",
    "\n",
    "# x = df['close']\n",
    "# y = df['close'].shift(1)\n",
    "# manhattan_distance = lambda x, y: np.abs(x - y)\n",
    "\n",
    "# d, cost_matrix, acc_cost_matrix, path = dtw(x, y, dist=manhattan_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For areas lacking data, can use masking \n",
    "\n",
    "inputs[:, 3, :] = 0. # for areas without a timestep, set to the mask value\n",
    "inputs[:, 5, :] = 0.\n",
    "model.add(tf.keras.layers.Masking(mask_value=0.,\n",
    "                                  input_shape=(timesteps, features)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
=======
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.constraints as constraints\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Activation, Add, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform, glorot_uniform\n",
    "\n",
    "\n",
    "def conv_block(X, filters, f, stage,block, strides=2):\n",
    "    \"\"\"\n",
    "    X -- input tensor\n",
    "    f -- interger specify shape of conv window\n",
    "    stage - integer, used to named layers, depending on position in network.\n",
    "    stride -- stride\n",
    "    \"\"\"\n",
    "    conv_base_name = 'conv' + str(stage) + block\n",
    "    bn_base_name = 'bn' + str(stage) + block\n",
    "    \n",
    "    f1, f2, f3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    X = Conv1D(f1, kernel_size=1, strides=strides, name=conv_base_name + '_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f2, kernel_size=f, strides=1, padding='same', name=conv_base_name + '_2', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f3, kernel_size=1, strides=1, padding='valid', name=conv_base_name + '_3', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # skip connection\n",
    "    X_shortcut = Conv1D(f3, kernel_size=1, strides=strides, padding='valid', name=conv_base_name + '_4', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    \n",
    "    assert tf.keras.backend.int_shape(X) == tf.keras.backend.int_shape(X_shortcut)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('tanh')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def identity_block(X, f, filters, stage, block, strides=2):\n",
    "    \"\"\"\n",
    "    X -- input tensor\n",
    "    f -- interger specify shape of conv window\n",
    "    stage - integer, used to named layers, depending on position in network.\n",
    "    stride -- stride\n",
    "    \"\"\"\n",
    "    conv_base_name = 'conv' + str(stage) + block\n",
    "    bn_base_name = 'bn' + str(stage) + block\n",
    "    \n",
    "    f1, f2, f3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    X = Conv1D(f1, kernel_size=1, strides=strides, name=conv_base_name + '_1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f2, kernel_size=f, strides=1, padding='same', name=conv_base_name + '_2', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = Activation('tanh')(X)\n",
    "    \n",
    "    X = Conv1D(f3, kernel_size=1, strides=strides, padding='valid', name=conv_base_name + '_3', \n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # skip connection, without also convolving x_shortcut\n",
    "    assert tf.keras.backend.int_shape(X) == tf.keras.backend.int_shape(X_shortcut)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('tanh')(X)\n",
    "    return X\n",
    "\n",
    "#class attention_layer(layers.Layer):\n",
    "#    #print([var.name for var in attn_layer.trainable_variables])\n",
    "#    def __init__(self, attention_size):\n",
    "#        super(attention_layer, self).__init__()\n",
    "#        self.attention_size = attention_size\n",
    "#        self.b_omega = tf.Variable(tf.random.normal([attention_size], stddev=0.1))\n",
    "#        self.u_omega = tf.Variable(tf.random.normal([attention_size], stddev=0.1))\n",
    "#        \n",
    "#    def build(self, input_shape):\n",
    "#        self.w_omega = tf.Variable(tf.random.normal([input_shape[-1], self.attention_size], stddev=0.1))\n",
    "#        \n",
    "#    def call(self, _in):\n",
    "#        v = tf.tanh(tf.tensordot(_in, self.w_omega, axes=1) + self.b_omega)\n",
    "#        vu = tf.tensordot(v, self.u_omega, axes=1, name='vu') # B,T shape\n",
    "#        alphas = tf.nn.softmax(vu, name='alphas')\n",
    "#        \n",
    "#        output = tf.reduce_sum(_in * tf.expand_dims(alphas, -1), 1)\n",
    "#        return output\n",
    "\n",
    "def create_model(x_train, params):\n",
    "    input_shape = x_train.shape[-2:]\n",
    "    X_input = Input(input_shape, batch_size=batch_size)\n",
    "    X = X_input\n",
    "    X = layers.ZeroPadding1D(padding=2)(X)\n",
    "#    X = layers.TimeDistributed(Dense(input_shape[-1], kernel_constraint=constraints.max_norm(1.0), activation='tanh'))(X)\n",
    "    X = Conv1D(filters=params.filters1, kernel_size=6, strides=1, name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = layers.GaussianNoise(params.noise1)(X)\n",
    "    #X = layers.AveragePooling1D(2, strides=1)(X)\n",
    "    X = conv_block(X, f=3, filters=params.filtershape1, stage=2, block='a', strides=2)\n",
    "    X = layers.GaussianNoise(params.noise1)(X)\n",
    "    X = identity_block(X, f=3, filters=params.filtershape1, stage=2, block='b', strides=1)\n",
    "    X = layers.GaussianNoise(params.noise1)(X)\n",
    "    X = identity_block(X, f=3, filters=params.filtershape1, stage=2, block='c', strides=1)\n",
    "    \n",
    "    print(X.get_shape())\n",
    "    print(X_input.get_shape())\n",
    "    #X = layers.concatenate([X, X_input])\n",
    "    \n",
    "   # X = layers.AveragePooling1D(2, name=\"avg_pool\")(X)\n",
    "    \n",
    "#    rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]\n",
    "#    stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)\n",
    "#    lstm_layer = tf.keras.layers.RNN(stacked_lstm)\n",
    "#    X = lstm_layer(X)\n",
    " \n",
    "    X = layers.Flatten()(X)\n",
    "    X = Dense(256)(X)\n",
    "    X = Dense(lahead, kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    model = Model(inputs=X_input, outputs=X, name='model1')\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptolytic.util import *\n",
    "        \n",
    "    \n",
    "def fit_model(model, inputX, inputy):\n",
    "    epochs = 5\n",
    "    # batch size higher than 1 c  epochs = 10\n",
    "    for i in range(epochs):\n",
    "        model.fit(inputX,\n",
    "                  inputy, \n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  verbose=1,\n",
    "                  use_multiprocessing=True,\n",
    "                  shuffle=False,\n",
    "                  workers=4,\n",
    "                  validation_data = (x_val, y_val))\n",
    "        history['loss'].append(model.history.history['loss'])\n",
    "        history['val_loss'].append(model.history.history['val_loss'])\n",
    "#        model.reset_states()\n",
    "    #pred = transformer.denormalize(model.predict(x_val)[:, 0], df, 'close')\n",
    "    #pred_history.append(pred)\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def save_model(model, folder, params=None):\n",
    "    path = f'models/{folder}'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    filename = os.path.join(path, 'model_' + str(np.random.rand()) + '.h5')\n",
    "    if os.path.exists(filename):\n",
    "        return save_model(model)\n",
    "    if params is not None:\n",
    "        param_path = os.path.join(path, 'model_params.csv')\n",
    "        pd.DataFrame(params).to_csv(param_path)\n",
    "        \n",
    "    model.save(filename)\n",
    "    print('Saved model')\n",
    "    \n",
    "def load_all_models(folder):\n",
    "    models = []\n",
    "    params = []\n",
    "    path = f'models/{folder}'\n",
    "    for m in os.listdir(path):\n",
    "        if m.endswith('.csv'):\n",
    "            params.append(pd.read_csv(m))\n",
    "        if not m.endswith('.h5'):\n",
    "            continue\n",
    "        m = tf.keras.models.load_model(m)\n",
    "        models.append(m)\n",
    "        print('Loaded %s' % path)\n",
    "    return models, params\n",
    "\n",
    "def stacked_dataset(models, inputX):\n",
    "    stackX = None\n",
    "    for model in models:\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        #stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilties]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "def fit_stacked_model(models, inputX, inputy):\n",
    "    stackedX = stacked_dataset(models, inputX)\n",
    "    # fit model\n",
    "    model = create_model(inputX)\n",
    "    fit_model(model, inputX, inputy)\n",
    "    return model\n",
    "\n",
    "def stacked_prediction(models, model, inputX):\n",
    "    stackedX = stacked_dataset(models, inputX)\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat\n",
    "\n",
    "def endingly():\n",
    "    models, params = load_all_models('example')\n",
    "    model = fit_stacked_model(models, x_train, y_train)\n",
    "    preds = stacked_prediction(models, model, x_train)\n",
    "    evaluate_models(models)\n",
    "    return preds\n",
    "\n",
    "def hyperparameter(inputX, inputy):\n",
    "    filtershape1 = 32 + 16 * np.random.randint(0, 6)\n",
    "    filtershape2 = 32 + 16 * np.random.randint(0, 6)\n",
    "    \n",
    "    params = adict(\n",
    "        filters1 = 32 + 16 * np.random.randint(0, 5),\n",
    "        noise1 = np.random.uniform(high=0.01),\n",
    "        filtershape1 = [filtershape1, filtershape1, filtershape1*2],\n",
    "        filtershape2 = [filtershape2, filtershape2, filtershape2*2]\n",
    "    )\n",
    "    print(params)\n",
    "    model = create_model(inputX, params)\n",
    "    fit_model(model, inputX, inputy)\n",
    "    save_model(model, 'filter', params=params)\n",
    "    return model\n",
    "                 \n",
    "def run_tuning():\n",
    "    models = []\n",
    "    for i in range(50):\n",
    "        models.append(hyperparameter(x_train, y_train))\n",
    "    return models\n",
    "\n",
    "   \n",
    "def evaluate_models(models):\n",
    "    for m in models:\n",
    "        _, acc = model.evaluate(x_val, y_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'filters1': 32, 'noise1': 0.007044669933974564, 'filtershape1': [48, 48, 96], 'filtershape2': [64, 64, 128]}\n",
    "{'filters1': 96, 'noise1': 0.0067625880989959294, 'filtershape1': [48, 48, 96], 'filtershape2': [32, 32, 64]}\n",
    "{'filters1': 64, 'noise1': 0.002182333479015578, 'filtershape1': [32, 32, 64], 'filtershape2': [48, 48, 96]}\n",
    "{'filters1': 48, 'noise1': 0.004816923153178217, 'filtershape1': [48, 48, 96], 'filtershape2': [32, 32, 64]}\n",
    "{'filters1': 32, 'noise1': 0.0004049353180822357, 'filtershape1': [64, 64, 128], 'filtershape2': [32, 32, 64]}\n",
    "{'filters1': 64, 'noise1': 0.0015181074459694344, 'filtershape1': [48, 48, 96], 'filtershape2': [64, 64, 128]}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.mean([32, 96, 64, 48, 32, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filters1': 96, 'noise1': 0.0023524256572771854, 'filtershape1': [64, 64, 128], 'filtershape2': [48, 48, 96]}\n",
      "(200, 100, 128)\n",
      "(200, 200, 115)\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 56s 5ms/sample - loss: 1.0337 - val_loss: 0.7323\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 53s 4ms/sample - loss: 0.5027 - val_loss: 0.5628\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 51s 4ms/sample - loss: 0.3414 - val_loss: 0.4160\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 53s 4ms/sample - loss: 0.2604 - val_loss: 0.4821\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 53s 4ms/sample - loss: 0.2079 - val_loss: 0.3329\n",
      "Saved model\n",
      "{'filters1': 32, 'noise1': 0.0073190999447795444, 'filtershape1': [32, 32, 64], 'filtershape2': [32, 32, 64]}\n",
      "(200, 100, 64)\n",
      "(200, 200, 115)\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 30s 2ms/sample - loss: 0.7505 - val_loss: 0.5962\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 28s 2ms/sample - loss: 0.3619 - val_loss: 0.5170\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      "12400/12400 [==============================] - 27s 2ms/sample - loss: 0.2518 - val_loss: 0.4510\n",
      "Train on 12400 samples, validate on 3000 samples\n",
      " 2200/12400 [====>.........................] - ETA: 20s - loss: 0.3306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-30-fb74b19115ac>\", line 1, in <module>\n",
      "    models = run_tuning()\n",
      "  File \"<ipython-input-19-8146e7a3eb85>\", line 101, in run_tuning\n",
      "    models.append(hyperparameter(x_train, y_train))\n",
      "  File \"<ipython-input-19-8146e7a3eb85>\", line 94, in hyperparameter\n",
      "    fit_model(model, inputX, inputy)\n",
      "  File \"<ipython-input-19-8146e7a3eb85>\", line 16, in fit_model\n",
      "    validation_data = (x_val, y_val))\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\n",
      "    total_epochs=epochs)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\n",
      "    batch_outs = execution_function(iterator)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\n",
      "    distributed_function(input_fn))\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 487, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\n",
      "    ctx=ctx)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\n",
      "    num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/me/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/me/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/me/anaconda3/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/me/anaconda3/lib/python3.7/inspect.py\", line 70, in ismodule\n",
      "    return isinstance(object, types.ModuleType)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "models = run_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "endingly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(x_train)\n",
    "history = {'loss':[],'val_loss':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils.vis_utils import model_to_dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "    \n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "        label='True Future')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "            label='Predicted Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_train_history(loss, val_loss, title):\n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_train_history(history['loss'], history['val_loss'], 'Multi Step Training and validation loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = denormalize(model.predict(x_train)[:, 0], df, 'close')\n",
    "#preds = denormalize(df, transformer)\n",
    "#preds = denormalize(model.predict(x_train)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20,9\n",
    "preds = model.predict(x_train)[:, 0]\n",
    "n = len(preds)\n",
    "h = history_size\n",
    "hn = n+history_size\n",
    "yo= len(x_val)+history_size\n",
    "z = yo\n",
    "modman = len(df)% batch_size\n",
    "w = z + len(df)%modman\n",
    "\n",
    "# plt.plot(range(n), d.denoise(df['close'][h:hn], 400), label='actual', color='green')\n",
    "# plt.plot(range(n), d.denoise(preds[:n], 5), label='predicted');\n",
    "# plt.plot(range(n), df.close_mean.values[h:hn], label='Mean')\n",
    "\n",
    "plt.plot(y_train[:, 0], label='Train')\n",
    "plt.plot(d.denoise(preds, 5), label='Preds')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = denormalize(model.predict(x_val)[:, 0], df, 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = denormalize(y_val[:, 0], df, 'close')\n",
    "#preds = model.predict(x_val)[:, 0]\n",
    "val_preds = model.predict(x_val)[:, 0]\n",
    "#y_actual = y_val[:, 0]\n",
    "val_n = len(val_preds)\n",
    "hn = n+history_size\n",
    "plt.plot(range(val_n), d.denoise(y_actual, 5), label='actual')\n",
    "#plt.plot(range(val_n), df['close'].iloc[-val_n:], label='actual')\n",
    "plt.plot(range(val_n), d.denoise(val_preds, 5), label='predicted');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(val_n), d.denoise(val_preds[1], 5), label='predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "yo = denormalize(model.predict(x_val)[:, 0], df, 'close')\n",
    "y_actual = denormalize(y_val[:, 0], df, 'close')\n",
    "plt.plot(yo, label='predicted')\n",
    "plt.plot(y_actual, label='actual')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}